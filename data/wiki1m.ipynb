{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mapping dataset takes a long (~15min) time, it is done separately and then saved, so that the training can be done by directly loading the already mapped dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following SimCSE procedure.\n",
    "import pandas as pd\n",
    "from datasets import Dataset,load_dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download wiki1m_for_simCSE.txt:\n",
    "https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to read simCSE-wiki.txt\n",
    "wiki_text_file = 'your-path-to/simCSE-wiki.txt'\n",
    "wiki = pd.read_csv(wiki_text_file,sep = '\\t',header = None)\n",
    "wiki.columns = ['text']\n",
    "# use Dataset.from_pandas to convert to dataset\n",
    "wiki_dataset = Dataset.from_pandas(wiki,split= \"train\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    \n",
    "    total = len(examples['text'])\n",
    "    # total = batch_size\n",
    "    \n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "\n",
    "    sentences = examples['text'] + examples['text']\n",
    "\n",
    "    # set max_length here:\n",
    "    sent_features = tokenizer(sentences, max_length=32, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    features = {}\n",
    "    for key in sent_features:\n",
    "        features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = wiki_dataset.map(prepare_features,batched=True, remove_columns=['text'], batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk for reuse\n",
    "train_dataset.save_to_disk(\"wiki_for_sts_32\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
